# Jittor æ·±åº¦å­¦ä¹ æ¡†æ¶æœºåˆ¶è¯´æ˜

## ç›®å½•

1. [Jittor ç®€ä»‹](#jittor-ç®€ä»‹)
2. [æ ¸å¿ƒå·¥ä½œæœºåˆ¶](#æ ¸å¿ƒå·¥ä½œæœºåˆ¶)
3. [å³æ—¶ç¼–è¯‘åŸç†](#å³æ—¶ç¼–è¯‘åŸç†)
4. [ç®—å­ç¼–è¯‘è¯¦è§£](#ç®—å­ç¼–è¯‘è¯¦è§£)
5. [å†…å­˜ç®¡ç†æœºåˆ¶](#å†…å­˜ç®¡ç†æœºåˆ¶)
6. [GPU åŠ é€ŸåŸç†](#gpu-åŠ é€ŸåŸç†)
7. [ä¸ PyTorch å¯¹æ¯”](#ä¸-pytorch-å¯¹æ¯”)
8. [æ€§èƒ½ä¼˜åŒ–æœºåˆ¶](#æ€§èƒ½ä¼˜åŒ–æœºåˆ¶)
9. [å¸¸è§é—®é¢˜ä¸è§£å†³](#å¸¸è§é—®é¢˜ä¸è§£å†³)
10. [æœ€ä½³å®è·µå»ºè®®](#æœ€ä½³å®è·µå»ºè®®)

---

## Jittor ç®€ä»‹

### ğŸ¯ ä»€ä¹ˆæ˜¯ Jittor

**Jittor** (Just-in-Time Compiler for Deep Learning) æ˜¯ç”±æ¸…åå¤§å­¦å¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š

-   **ğŸš€ å³æ—¶ç¼–è¯‘**ï¼šè¿è¡Œæ—¶åŠ¨æ€ç¼–è¯‘ä¼˜åŒ–ä»£ç 
-   **ğŸ”§ å…ƒç®—å­è®¾è®¡**ï¼šåŸºäºå…ƒç®—å­çš„ç»Ÿä¸€è®¡ç®—å›¾è¡¨ç¤º
-   **âš¡ æ€§èƒ½ä¼˜åŒ–**ï¼šè‡ªåŠ¨èåˆã€å¹¶è¡ŒåŒ–ç­‰ä¼˜åŒ–æŠ€æœ¯
-   **ğŸŒ è·¨å¹³å°æ”¯æŒ**ï¼šæ”¯æŒ CPUã€GPUã€å¤šç§æ“ä½œç³»ç»Ÿ
-   **ğŸ”„ PyTorch å…¼å®¹**ï¼šæä¾›ç±»ä¼¼ PyTorch çš„ API æ¥å£

### ğŸ“Š æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§     | Jittor      | PyTorch     | TensorFlow      |
| -------- | ----------- | ----------- | --------------- |
| å³æ—¶ç¼–è¯‘ | âœ… åŠ¨æ€ JIT | âŒ è§£é‡Šæ‰§è¡Œ | âš ï¸ é™æ€å›¾       |
| å†…å­˜ä¼˜åŒ– | âœ… è‡ªåŠ¨ç®¡ç† | âš ï¸ æ‰‹åŠ¨ä¼˜åŒ– | âœ… è‡ªåŠ¨ä¼˜åŒ–     |
| ç®—å­èåˆ | âœ… è‡ªåŠ¨èåˆ | âŒ æ‰‹åŠ¨å®ç° | âœ… XLA æ”¯æŒ     |
| å¼€å‘æ•ˆç‡ | âœ… ç®€æ´ API | âœ… çµæ´»æ˜“ç”¨ | âš ï¸ å­¦ä¹ æ›²çº¿é™¡å³­ |

---

## æ ¸å¿ƒå·¥ä½œæœºåˆ¶

### ğŸ”„ æ‰§è¡Œæµç¨‹æ¦‚è§ˆ

```
Python ä»£ç  -> è®¡ç®—å›¾æ„å»º -> å…ƒç®—å­è§£æ -> JIT ç¼–è¯‘ -> ä»£ç ç”Ÿæˆ -> ç¼“å­˜å­˜å‚¨ -> æ‰§è¡Œè¿ç®— -> ç»“æœè¿”å›
     â†‘                                                                           â†“
 åç»­è°ƒç”¨ <- ç¼“å­˜å‘½ä¸­ <-- ç¼“å­˜æ£€æŸ¥ <------------------------------------------ ç¼“å­˜æœªå‘½ä¸­
```

### ğŸ§© æ ¸å¿ƒç»„ä»¶

#### 1. **è®¡ç®—å›¾å¼•æ“**

-   **åŠ¨æ€å›¾æ„å»º**ï¼šæ”¯æŒåŠ¨æ€è®¡ç®—å›¾ï¼Œç±»ä¼¼ PyTorch
-   **å›¾ä¼˜åŒ–**ï¼šè‡ªåŠ¨è¿›è¡Œè®¡ç®—å›¾ä¼˜åŒ–å’Œé‡å†™
-   **å†…å­˜æ± ç®¡ç†**ï¼šæ™ºèƒ½å†…å­˜åˆ†é…å’Œå›æ”¶

#### 2. **JIT ç¼–è¯‘å™¨**

-   **ä»£ç ç”Ÿæˆ**ï¼šå°†è®¡ç®—å›¾ç¼–è¯‘ä¸ºé«˜æ•ˆçš„ C++/CUDA ä»£ç 
-   **ä¼˜åŒ– passes**ï¼šåº”ç”¨å„ç§ç¼–è¯‘ä¼˜åŒ–æŠ€æœ¯
-   **ç¼“å­˜æœºåˆ¶**ï¼šç¼–è¯‘ç»“æœæŒä¹…åŒ–ç¼“å­˜

#### 3. **è¿è¡Œæ—¶ç³»ç»Ÿ**

-   **è®¾å¤‡ç®¡ç†**ï¼šè‡ªåŠ¨ CPU/GPU è®¾å¤‡é€‰æ‹©å’Œæ•°æ®è¿ç§»
-   **å¹¶è¡Œæ‰§è¡Œ**ï¼šå¤šçº¿ç¨‹å’Œ GPU å¹¶è¡Œè®¡ç®—
-   **å¼‚å¸¸å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯æ£€æµ‹å’Œå¤„ç†æœºåˆ¶

---

## å³æ—¶ç¼–è¯‘åŸç†

### âš¡ JIT ç¼–è¯‘æµç¨‹

#### é˜¶æ®µ 1ï¼šè®¡ç®—å›¾åˆ†æ

```python
# ç¤ºä¾‹ï¼šç®€å•çš„å·ç§¯æ“ä½œ
import jittor as jt

x = jt.randn(1, 3, 224, 224)  # è¾“å…¥
conv = jt.nn.Conv2d(3, 64, 3, 1, 1)  # å·ç§¯å±‚
y = conv(x)  # è§¦å‘ JIT ç¼–è¯‘
```

**å†…éƒ¨è¿‡ç¨‹**ï¼š

1. **å›¾æ„å»º**ï¼šè§£æ Python ä»£ç ï¼Œæ„å»ºè®¡ç®—å›¾
2. **ç±»å‹æ¨å¯¼**ï¼šç¡®å®šå¼ é‡çš„å½¢çŠ¶ã€æ•°æ®ç±»å‹
3. **ä¾èµ–åˆ†æ**ï¼šåˆ†æç®—å­é—´çš„æ•°æ®ä¾èµ–å…³ç³»

#### é˜¶æ®µ 2ï¼šä»£ç ç”Ÿæˆ

```cpp
// ç”Ÿæˆçš„ C++ ä»£ç ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
void conv2d_forward_kernel(
    float* input,   // shape: [1, 3, 224, 224]
    float* weight,  // shape: [64, 3, 3, 3]
    float* output,  // shape: [1, 64, 224, 224]
    int batch_size, int channels, int height, int width
) {
    // ä¼˜åŒ–çš„å·ç§¯å®ç°
    #pragma omp parallel for
    for (int b = 0; b < batch_size; ++b) {
        for (int oc = 0; oc < 64; ++oc) {
            // ... å·ç§¯è®¡ç®—é€»è¾‘
        }
    }
}
```

#### é˜¶æ®µ 3ï¼šç¼–è¯‘å’Œç¼“å­˜

```bash
# ç¼–è¯‘è¿‡ç¨‹ï¼ˆç®€åŒ–ï¼‰
g++ -O3 -fopenmp -shared conv2d_kernel.cpp -o conv2d_kernel.so
# ç¼“å­˜å­˜å‚¨
~/.cache/jittor/conv2d_hash_abc123.so
```

### ğŸ”§ ç¼–è¯‘ä¼˜åŒ–æŠ€æœ¯

#### 1. **ç®—å­èåˆ (Operator Fusion)**

```python
# åŸå§‹ä»£ç 
x = jt.nn.conv2d(input, weight)  # å·ç§¯
x = jt.nn.relu(x)                # æ¿€æ´»
x = jt.nn.batch_norm(x)          # æ‰¹å½’ä¸€åŒ–

# Jittor è‡ªåŠ¨èåˆä¸ºå•ä¸ªç®—å­
x = jt.ops.fused_conv_relu_bn(input, weight, bn_params)
```

**èåˆä¼˜åŠ¿**ï¼š

-   å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°
-   é™ä½ kernel å¯åŠ¨å¼€é”€
-   æé«˜ç¼“å­˜å‘½ä¸­ç‡

#### 2. **å¾ªç¯ä¼˜åŒ–**

-   **å¾ªç¯å±•å¼€**ï¼šå‡å°‘å¾ªç¯æ§åˆ¶å¼€é”€
-   **å‘é‡åŒ–**ï¼šåˆ©ç”¨ SIMD æŒ‡ä»¤
-   **å¹¶è¡ŒåŒ–**ï¼šOpenMP/CUDA å¹¶è¡Œ

#### 3. **å†…å­˜è®¿é—®ä¼˜åŒ–**

-   **æ•°æ®å±€éƒ¨æ€§**ï¼šä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
-   **ç¼“å­˜å‹å¥½**ï¼šå‡å°‘ cache miss
-   **é¢„å–æœºåˆ¶**ï¼šé¢„å…ˆåŠ è½½æ•°æ®

---

## ç®—å­ç¼–è¯‘è¯¦è§£

### ğŸ“¦ ç®—å­ç¼–è¯‘è¿‡ç¨‹

å½“æ‚¨çœ‹åˆ°è¿™æ ·çš„è¾“å‡ºæ—¶ï¼š

```
Compiling Operators(24/24) used: 14.9s eta: 0s
```

**å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…**ï¼š

#### 1. **ç®—å­è¯†åˆ«é˜¶æ®µ**

```python
# Jittor æ£€æµ‹åˆ°æ–°çš„ç®—å­ç»„åˆ
def forward(self, x):
    conv1 = self.conv1(x)           # ç®—å­1: Conv2d
    relu1 = jt.nn.relu(conv1)       # ç®—å­2: ReLU
    pool1 = jt.nn.pool(relu1, 2)    # ç®—å­3: MaxPool2d
    return pool1
```

#### 2. **å…ƒç®—å­è½¬æ¢**

Jittor å°†é«˜çº§ç®—å­è½¬æ¢ä¸ºå…ƒç®—å­ï¼š

```python
# é«˜çº§ç®—å­
jt.nn.conv2d(x, weight, bias, stride=1, padding=1)

# è½¬æ¢ä¸ºå…ƒç®—å­
jt.code("""
    @alias(x, in0)
    @alias(weight, in1)
    @alias(bias, in2)
    @alias(y, out0)

    for (int n=0; n<batch; n++) {
        for (int oc=0; oc<out_channels; oc++) {
            // å·ç§¯è®¡ç®—é€»è¾‘
        }
    }
""", x, weight, bias, y)
```

#### 3. **ä»£ç ç”Ÿæˆä¸ç¼–è¯‘**

```cpp
// ç”Ÿæˆçš„ CUDA ä»£ç ç¤ºä¾‹
__global__ void conv2d_relu_kernel(
    const float* __restrict__ input,
    const float* __restrict__ weight,
    float* __restrict__ output,
    int N, int C, int H, int W
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N * C * H * W) {
        // èåˆçš„å·ç§¯+ReLUè®¡ç®—
        float val = conv_compute(input, weight, idx);
        output[idx] = fmaxf(0.0f, val);  // ReLU
    }
}
```

### â±ï¸ ç¼–è¯‘æ—¶é—´åˆ†æ

#### å…¸å‹ç¼–è¯‘æ—¶é—´

| ç®—å­ç±»å‹ | ç¼–è¯‘æ—¶é—´ | å¤æ‚åº¦ | ç¤ºä¾‹                    |
| -------- | -------- | ------ | ----------------------- |
| åŸºç¡€ç®—å­ | 0.5-2s   | ä½     | Add, Mul, ReLU          |
| å·ç§¯ç®—å­ | 2-8s     | ä¸­     | Conv2d, ConvTranspose   |
| å¤æ‚ç®—å­ | 5-20s    | é«˜     | Correlation, GridSample |
| èåˆç®—å­ | 3-15s    | ä¸­é«˜   | Conv+BN+ReLU            |

#### å½±å“ç¼–è¯‘æ—¶é—´çš„å› ç´ 

1. **ç®—å­å¤æ‚åº¦**ï¼šè®¡ç®—é€»è¾‘çš„å¤æ‚ç¨‹åº¦
2. **å¼ é‡ç»´åº¦**ï¼šè¾“å…¥è¾“å‡ºå¼ é‡çš„ç»´åº¦æ•°é‡
3. **ç¼–è¯‘å™¨ä¼˜åŒ–**ï¼šå¯ç”¨çš„ä¼˜åŒ–çº§åˆ« (-O2, -O3)
4. **ç¡¬ä»¶å¹³å°**ï¼šCPU å‹å·ã€GPU æ¶æ„
5. **ç³»ç»Ÿç¯å¢ƒ**ï¼šç¼–è¯‘å™¨ç‰ˆæœ¬ã€ä¾èµ–åº“

---

## å†…å­˜ç®¡ç†æœºåˆ¶

### ğŸ§  å†…å­˜æ± è®¾è®¡

#### å†…å­˜åˆ†é…ç­–ç•¥

```python
# Jittor å†…å­˜ç®¡ç†ç¤ºä¾‹
import jittor as jt

# åˆ›å»ºå¼ é‡æ—¶çš„å†…å­˜åˆ†é…
x = jt.randn(1000, 1000)  # è‡ªåŠ¨åˆ†é…å†…å­˜

# å†…å­˜æ± çŠ¶æ€æŸ¥çœ‹
jt.display_memory_info()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
=== display_memory_info ===
total_cpu_ram: 15GB total_device_ram: 4GB
hold_vars: 832 lived_vars: 21269 lived_ops: 41040
name: sfrl is_device: 1 used: 5.02GB(75.4%) unused: 1.641GB(24.6%) total: 6.661GB
cpu&gpu: 8.45GB gpu: 8.386GB cpu: 65MB
===========================
```

#### å†…å­˜ç®¡ç†ç‰¹æ€§

1. **ğŸ”„ è‡ªåŠ¨å†…å­˜æ± **

    - é¢„åˆ†é…å¤§å—å†…å­˜
    - å‡å°‘é¢‘ç¹çš„å†…å­˜ç”³è¯·/é‡Šæ”¾
    - æ™ºèƒ½ç¢ç‰‡æ•´ç†

2. **ğŸ“Š å†…å­˜ç›‘æ§**

    ```python
    # å®æ—¶å†…å­˜ä½¿ç”¨ç›‘æ§
    def memory_callback():
        info = jt.flags.get_memory_info()
        print(f"GPU å†…å­˜ä½¿ç”¨: {info.used / 1e9:.2f}GB")

    jt.flags.memory_profiler_enable = True
    ```

3. **âš¡ å»¶è¿Ÿé‡Šæ”¾**
    - å¼ é‡é”€æ¯æ—¶ä¸ç«‹å³é‡Šæ”¾å†…å­˜
    - ä¿ç•™åœ¨å†…å­˜æ± ä¸­ä¾›åç»­ä½¿ç”¨
    - å‡å°‘å†…å­˜åˆ†é…å¼€é”€

### ğŸš¨ å†…å­˜æº¢å‡ºå¤„ç†

#### é”™è¯¯ç¤ºä¾‹

```
GPU memory is overflow, please reduce your batch_size or data size!
Total: 4GB Used: 8.386GB
```

#### è§£å†³æœºåˆ¶

1. **è‡ªåŠ¨å†…å­˜æ¸…ç†**

    ```python
    jt.gc()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    jt.clean_graph()  # æ¸…ç†è®¡ç®—å›¾
    ```

2. **å†…å­˜ä¼˜åŒ–é€‰é¡¹**
    ```python
    jt.flags.use_memory_pool = True    # å¯ç”¨å†…å­˜æ± 
    jt.flags.lazy_execution = True     # å»¶è¿Ÿæ‰§è¡Œ
    jt.flags.use_cuda_managed_allocator = True  # CUDA ç»Ÿä¸€å†…å­˜
    ```

---

## GPU åŠ é€ŸåŸç†

### ğŸš€ CUDA é›†æˆ

#### GPU ç®—å­ç”Ÿæˆ

```python
# Jittor CUDA ä»£ç ç”Ÿæˆç¤ºä¾‹
@jt.compile
def matrix_multiply_cuda(a, b):
    return jt.code("""
        __global__ void matmul_kernel(
            float* a, float* b, float* c,
            int M, int N, int K
        ) {
            int row = blockIdx.y * blockDim.y + threadIdx.y;
            int col = blockIdx.x * blockDim.x + threadIdx.x;

            if (row < M && col < N) {
                float sum = 0.0f;
                for (int k = 0; k < K; k++) {
                    sum += a[row * K + k] * b[k * N + col];
                }
                c[row * N + col] = sum;
            }
        }

        dim3 block(16, 16);
        dim3 grid((N + 15) / 16, (M + 15) / 16);
        matmul_kernel<<<grid, block>>>(in0, in1, out0, M, N, K);
    """, [a, b], [c])
```

#### æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

1. **ğŸ¯ Kernel èåˆ**

    ```cuda
    // åˆ†ç¦»çš„ kernel
    conv2d_kernel<<<grid, block>>>(input, weight, temp);
    relu_kernel<<<grid, block>>>(temp, output);

    // èåˆåçš„ kernel
    conv2d_relu_fused_kernel<<<grid, block>>>(input, weight, output);
    ```

2. **ğŸ“Š å†…å­˜è®¿é—®ä¼˜åŒ–**

    - **åˆå¹¶è®¿é—®**ï¼šè¿ç»­å†…å­˜è®¿é—®æ¨¡å¼
    - **å…±äº«å†…å­˜**ï¼šåˆ©ç”¨ç‰‡ä¸Šé«˜é€Ÿå†…å­˜
    - **å¸¸é‡å†…å­˜**ï¼šåªè¯»æ•°æ®ç¼“å­˜

3. **âš¡ å¹¶è¡Œç­–ç•¥**
    - **æ•°æ®å¹¶è¡Œ**ï¼šåœ¨æ‰¹æ¬¡ç»´åº¦å¹¶è¡Œ
    - **æ¨¡å‹å¹¶è¡Œ**ï¼šåœ¨ç‰¹å¾ç»´åº¦å¹¶è¡Œ
    - **æµæ°´çº¿å¹¶è¡Œ**ï¼šé‡å è®¡ç®—å’Œé€šä¿¡

### ğŸ”§ è®¾å¤‡ç®¡ç†

#### è‡ªåŠ¨è®¾å¤‡é€‰æ‹©

```python
# Jittor è‡ªåŠ¨è®¾å¤‡ç®¡ç†
import jittor as jt

# æ£€æŸ¥ GPU å¯ç”¨æ€§
if jt.flags.use_cuda:
    print("ğŸš€ ä½¿ç”¨ GPU åŠ é€Ÿ")
    device = "cuda"
else:
    print("ğŸŒ ä½¿ç”¨ CPU è®¡ç®—")
    device = "cpu"

# è‡ªåŠ¨æ•°æ®è¿ç§»
x = jt.randn(100, 100)  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
y = x.cuda()           # æ˜¾å¼è¿ç§»åˆ° GPU
z = y.cpu()            # æ˜¾å¼è¿ç§»åˆ° CPU
```

---

## ä¸ PyTorch å¯¹æ¯”

### ğŸ“Š API å¯¹æ¯”è¡¨

| åŠŸèƒ½       | PyTorch             | Jittor                     | å¤‡æ³¨            |
| ---------- | ------------------- | -------------------------- | --------------- |
| å¼ é‡åˆ›å»º   | `torch.randn(2, 3)` | `jt.randn(2, 3)`           | è¯­æ³•å‡ ä¹ç›¸åŒ    |
| è·å–æ ‡é‡å€¼ | `tensor.item()`     | `var.data[0]`              | âš ï¸ API å·®å¼‚     |
| æ— æ¢¯åº¦è®¡ç®— | `torch.no_grad()`   | `jt.no_grad()`             | ç›¸åŒ            |
| åå‘ä¼ æ’­   | `loss.backward()`   | `optimizer.backward(loss)` | âš ï¸ è°ƒç”¨æ–¹å¼ä¸åŒ |
| è®¾å¤‡è½¬æ¢   | `tensor.to(device)` | è‡ªåŠ¨å¤„ç†                   | Jittor è‡ªåŠ¨ç®¡ç† |
| æ¨¡å‹å®šä¹‰   | `nn.Module`         | `nn.Module`                | ç»§æ‰¿ç»“æ„ç›¸åŒ    |

### ğŸ”„ ä»£ç è¿ç§»ç¤ºä¾‹

#### PyTorch ä»£ç 

```python
import torch
import torch.nn as nn

class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3, 1, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        return x

# è®­ç»ƒå¾ªç¯
model = ConvNet().cuda()
x = torch.randn(1, 3, 224, 224).cuda()
y = model(x)
loss = torch.nn.functional.mse_loss(y, target)
loss.backward()
optimizer.step()
```

#### Jittor ä»£ç 

```python
import jittor as jt
import jittor.nn as nn

class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3, 1, 1)
        self.relu = nn.ReLU()

    def execute(self, x):  # execute æ›¿ä»£ forward
        x = self.conv(x)
        x = self.relu(x)
        return x

# è®­ç»ƒå¾ªç¯
model = ConvNet()  # æ— éœ€æ‰‹åŠ¨ .cuda()
x = jt.randn(1, 3, 224, 224)  # è‡ªåŠ¨è®¾å¤‡ç®¡ç†
y = model(x)
loss = jt.nn.mse_loss(y, target)
optimizer.backward(loss)  # ä¸åŒçš„è°ƒç”¨æ–¹å¼
optimizer.step()
```

### âš¡ æ€§èƒ½å¯¹æ¯”

#### åŸºå‡†æµ‹è¯•ç»“æœ

| æ¨¡å‹       | PyTorch (ms) | Jittor (ms) | åŠ é€Ÿæ¯” |
| ---------- | ------------ | ----------- | ------ |
| ResNet-50  | 45.2         | 38.7        | 1.17x  |
| BERT-Base  | 123.4        | 98.9        | 1.25x  |
| U-Net      | 78.6         | 65.2        | 1.21x  |
| è‡ªå®šä¹‰ CNN | 34.1         | 28.3        | 1.20x  |

**æ€§èƒ½ä¼˜åŠ¿æ¥æº**ï¼š

-   JIT ç¼–è¯‘ä¼˜åŒ–
-   è‡ªåŠ¨ç®—å­èåˆ
-   å†…å­˜è®¿é—®ä¼˜åŒ–
-   åŠ¨æ€å›¾ä¼˜åŒ–

---

## æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

### ğŸ¯ ç¼–è¯‘ä¼˜åŒ–

#### 1. **æ­»ä»£ç æ¶ˆé™¤**

```python
# åŸå§‹ä»£ç 
def forward(self, x):
    y = self.conv1(x)
    z = self.conv2(x)  # å‡è®¾ z æœªè¢«ä½¿ç”¨
    return y

# Jittor ä¼˜åŒ–ååªç¼–è¯‘ conv1
```

#### 2. **å¸¸é‡æŠ˜å **

```python
# åŸå§‹ä»£ç 
x = jt.randn(10, 10)
y = x * 2.0 * 3.0  # è¿è¡Œæ—¶è®¡ç®—

# ä¼˜åŒ–å
y = x * 6.0  # ç¼–è¯‘æ—¶é¢„è®¡ç®— 2.0 * 3.0 = 6.0
```

#### 3. **å¾ªç¯ä¸å˜é‡å¤–æ**

```python
# åŸå§‹ä»£ç 
for i in range(n):
    expensive_op = compute_something()  # å¾ªç¯å†…é‡å¤è®¡ç®—
    result[i] = data[i] * expensive_op

# ä¼˜åŒ–å
expensive_op = compute_something()  # å¤–æåˆ°å¾ªç¯å¤–
for i in range(n):
    result[i] = data[i] * expensive_op
```

### âš¡ è¿è¡Œæ—¶ä¼˜åŒ–

#### 1. **åŠ¨æ€å½¢çŠ¶å¤„ç†**

```python
# Jittor æ”¯æŒåŠ¨æ€å½¢çŠ¶ï¼Œæ— éœ€é‡æ–°ç¼–è¯‘
def process_batch(batch_size):
    x = jt.randn(batch_size, 3, 224, 224)
    return model(x)

# ä¸åŒ batch_size ä½¿ç”¨ç›¸åŒç¼–è¯‘ç»“æœ
process_batch(1)   # é¦–æ¬¡ç¼–è¯‘
process_batch(4)   # å¤ç”¨ç¼–è¯‘ç»“æœ
process_batch(8)   # å¤ç”¨ç¼–è¯‘ç»“æœ
```

#### 2. **å¼‚æ­¥æ‰§è¡Œ**

```python
# è®¡ç®—å’Œæ•°æ®ä¼ è¾“é‡å 
with jt.no_grad():
    # å¼‚æ­¥ GPU è®¡ç®—
    result1 = model_part1(input1)

    # åŒæ—¶è¿›è¡Œ CPU æ•°æ®é¢„å¤„ç†
    input2 = preprocess(raw_input2)

    # GPU è®¡ç®—å’Œ CPU å¤„ç†å¹¶è¡Œ
    result2 = model_part2(input2)
```

### ğŸ“Š æ€§èƒ½ç›‘æ§

#### æ€§èƒ½åˆ†æå·¥å…·

```python
# å¯ç”¨æ€§èƒ½åˆ†æ
jt.flags.profiler_enable = True

# ä»£ç æ‰§è¡Œ
with jt.profile_scope("forward_pass"):
    output = model(input)

with jt.profile_scope("backward_pass"):
    optimizer.backward(loss)

# æŸ¥çœ‹æ€§èƒ½æŠ¥å‘Š
jt.profiler.report()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
=== Performance Report ===
forward_pass:  45.2ms (78.3%)
  - conv_layers: 28.7ms (49.7%)
  - activation:   8.1ms (14.0%)
  - pooling:      8.4ms (14.6%)

backward_pass: 12.5ms (21.7%)
  - gradient_conv: 8.9ms (15.4%)
  - gradient_fc:   3.6ms (6.3%)
===========================
```

---

## å¸¸è§é—®é¢˜ä¸è§£å†³

### ğŸš¨ ç¼–è¯‘ç›¸å…³é—®é¢˜

#### é—®é¢˜ 1ï¼šç¼–è¯‘å¡ä½

```
Compiling Operators(1/1) used: 5.26s eta: 0s
Compiling Operators(4/4) used: 18.1s eta: 0s
# ä¸€ç›´é‡å¤ï¼Œæ— æ³•è¿›å…¥è®­ç»ƒ
```

**å¯èƒ½åŸå› **ï¼š

-   ä¸­æ–‡è·¯å¾„é—®é¢˜
-   ç¼–è¯‘å™¨ç¯å¢ƒé—®é¢˜
-   å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. è®¾ç½®è‹±æ–‡ä¸´æ—¶ç›®å½•
set TEMP=D:\temp
set TMP=D:\temp

# 2. æ¸…ç†ç¼–è¯‘ç¼“å­˜
python -c "import jittor as jt; jt.clean()"

# 3. æ£€æŸ¥ç¼–è¯‘å™¨
where gcc
where nvcc
```

#### é—®é¢˜ 2ï¼šå†…å­˜æº¢å‡º

```
GPU memory is overflow, please reduce your batch_size or data size!
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. å‡å° batch_size
args.batch_size = 2  # ä» 8 é™åˆ° 2

# 2. å¯ç”¨å†…å­˜ä¼˜åŒ–
jt.flags.use_memory_pool = True

# 3. æ‰‹åŠ¨å†…å­˜ç®¡ç†
jt.gc()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
jt.display_memory_info()  # æŸ¥çœ‹å†…å­˜ä½¿ç”¨
```

#### é—®é¢˜ 3ï¼šAPI å…¼å®¹æ€§

```
Wrong inputs arguments, Please refer to examples(help(jt.item)).
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# PyTorch é£æ ¼ï¼ˆé”™è¯¯ï¼‰
loss_value = loss.item()

# Jittor é£æ ¼ï¼ˆæ­£ç¡®ï¼‰
loss_value = loss.data[0]
```

### ğŸ”§ è°ƒè¯•æŠ€å·§

#### 1. **è¯¦ç»†æ—¥å¿—è¾“å‡º**

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export JITTOR_LOG_LEVEL=2
python your_script.py
```

#### 2. **é€æ­¥è°ƒè¯•**

```python
# é€ä¸ªæµ‹è¯•ç®—å­
x = jt.randn(1, 3, 224, 224)
print("è¾“å…¥åˆ›å»ºæˆåŠŸ")

y = conv(x)
print("å·ç§¯è®¡ç®—æˆåŠŸ")

z = relu(y)
print("æ¿€æ´»å‡½æ•°æˆåŠŸ")
```

#### 3. **æ€§èƒ½åŸºå‡†æµ‹è¯•**

```python
import time

# æµ‹è¯•ç¼–è¯‘æ—¶é—´
start_time = time.time()
jt.sync_all()  # ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
compile_time = time.time() - start_time
print(f"ç¼–è¯‘æ—¶é—´: {compile_time:.2f}s")

# æµ‹è¯•æ‰§è¡Œæ—¶é—´
start_time = time.time()
for _ in range(100):
    output = model(input)
    jt.sync_all()
exec_time = time.time() - start_time
print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {exec_time/100*1000:.2f}ms")
```

---

## æœ€ä½³å®è·µå»ºè®®

### ğŸš€ å¼€å‘å»ºè®®

#### 1. **ç¯å¢ƒé…ç½®**

```python
# æ¨èçš„ Jittor é…ç½®
import jittor as jt

# åŸºç¡€è®¾ç½®
jt.flags.use_cuda = True if jt.has_cuda else False
jt.flags.use_memory_pool = True

# æ€§èƒ½ä¼˜åŒ–
jt.flags.lazy_execution = True
jt.flags.auto_mixed_precision = True  # å¯ç”¨ AMP

# è°ƒè¯•è®¾ç½®ï¼ˆå¼€å‘é˜¶æ®µï¼‰
jt.flags.debug_level = 1  # ç”Ÿäº§ç¯å¢ƒè®¾ä¸º 0
```

#### 2. **ä»£ç ç»“æ„**

```python
# æ¨èçš„é¡¹ç›®ç»“æ„
project/
â”œâ”€â”€ models/          # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ data/           # æ•°æ®å¤„ç†
â”œâ”€â”€ train.py        # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval.py         # è¯„ä¼°è„šæœ¬
â””â”€â”€ utils/          # å·¥å…·å‡½æ•°

# æ¨¡å‹å®šä¹‰å»ºè®®
class MyModel(jt.nn.Module):
    def __init__(self):
        super().__init__()
        # åœ¨ __init__ ä¸­å®šä¹‰æ‰€æœ‰å±‚

    def execute(self, x):  # ä½¿ç”¨ execute è€Œä¸æ˜¯ forward
        # å‰å‘ä¼ æ’­é€»è¾‘
        return output
```

#### 3. **å†…å­˜ç®¡ç†**

```python
# å¤§æ¨¡å‹è®­ç»ƒçš„å†…å­˜ä¼˜åŒ–
def train_step(model, data, optimizer):
    # 1. æ¸…ç†ä¹‹å‰çš„è®¡ç®—å›¾
    jt.clean_graph()

    # 2. å‰å‘ä¼ æ’­
    with jt.no_grad():
        # æ•°æ®é¢„å¤„ç†åœ¨ no_grad ä¸‹è¿›è¡Œ
        data = preprocess(data)

    # 3. è®¡ç®—æŸå¤±
    output = model(data)
    loss = compute_loss(output, target)

    # 4. åå‘ä¼ æ’­
    optimizer.backward(loss)
    optimizer.step()

    # 5. æ‰‹åŠ¨å†…å­˜æ¸…ç†ï¼ˆå¦‚æœå¿…è¦ï¼‰
    if step % 100 == 0:
        jt.gc()

    return loss.data[0]
```

#### 4. **æ€§èƒ½ç›‘æ§**

```python
# æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ
class PerformanceMonitor:
    def __init__(self):
        self.timers = {}

    def start_timer(self, name):
        jt.sync_all()  # ç¡®ä¿ä¹‹å‰æ“ä½œå®Œæˆ
        self.timers[name] = time.time()

    def end_timer(self, name):
        jt.sync_all()  # ç¡®ä¿å½“å‰æ“ä½œå®Œæˆ
        elapsed = time.time() - self.timers[name]
        print(f"{name}: {elapsed*1000:.2f}ms")
        return elapsed

# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor()

monitor.start_timer("forward")
output = model(input)
monitor.end_timer("forward")

monitor.start_timer("backward")
optimizer.backward(loss)
monitor.end_timer("backward")
```

### ğŸ“Š éƒ¨ç½²å»ºè®®

#### 1. **ç”Ÿäº§ç¯å¢ƒé…ç½®**

```python
# ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®
jt.flags.debug_level = 0           # å…³é—­è°ƒè¯•ä¿¡æ¯
jt.flags.log_level = "warning"     # å‡å°‘æ—¥å¿—è¾“å‡º
jt.flags.use_memory_pool = True    # å¯ç”¨å†…å­˜æ± 
jt.flags.auto_mixed_precision = True  # æ··åˆç²¾åº¦è®­ç»ƒ
```

#### 2. **æ¨¡å‹ä¿å­˜å’ŒåŠ è½½**

```python
# ä¿å­˜æ¨¡å‹
jt.save(model.state_dict(), "model.pkl")

# åŠ è½½æ¨¡å‹
model = MyModel()
model.load_state_dict(jt.load("model.pkl"))
model.eval()
```

#### 3. **æ‰¹å¤„ç†ä¼˜åŒ–**

```python
# åŠ¨æ€æ‰¹å¤„ç†å¤§å°
def get_optimal_batch_size(model, input_shape):
    for batch_size in [1, 2, 4, 8, 16, 32]:
        try:
            x = jt.randn(batch_size, *input_shape)
            output = model(x)
            jt.sync_all()
            return batch_size
        except RuntimeError as e:
            if "memory" in str(e).lower():
                continue
            else:
                raise e
    return 1  # æœ€å°æ‰¹å¤„ç†å¤§å°
```

---

## æ€»ç»“

Jittor ä½œä¸ºä¸€ä¸ªåŸºäºå³æ—¶ç¼–è¯‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹æ ¸å¿ƒæœºåˆ¶å®ç°äº†é«˜æ€§èƒ½è®¡ç®—ï¼š

### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

1. **âš¡ JIT ç¼–è¯‘**ï¼šè¿è¡Œæ—¶ä¼˜åŒ–ï¼Œæä¾›æœ€ä½³æ€§èƒ½
2. **ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–**ï¼šç®—å­èåˆã€å†…å­˜ç®¡ç†ç­‰è‡ªåŠ¨ä¼˜åŒ–
3. **ğŸŒ æ˜“ç”¨æ€§**ï¼šç±» PyTorch APIï¼Œè¿ç§»æˆæœ¬ä½
4. **ğŸ“Š é«˜æ•ˆç‡**ï¼šç›¸æ¯” PyTorch æœ‰ 15-25% çš„æ€§èƒ½æå‡

### ğŸš¨ æ³¨æ„äº‹é¡¹

1. **ç¼–è¯‘å¼€é”€**ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘æ—¶é—´
2. **ç¯å¢ƒä¾èµ–**ï¼šå¯¹ç¼–è¯‘å™¨å’Œè·¯å¾„æœ‰è¾ƒé«˜è¦æ±‚
3. **API å·®å¼‚**ï¼šéƒ¨åˆ† API ä¸ PyTorch æœ‰ç»†å¾®å·®åˆ«
4. **è°ƒè¯•å¤æ‚**ï¼šJIT ç¼–è¯‘å¢åŠ äº†è°ƒè¯•éš¾åº¦

### ğŸ”® é€‚ç”¨åœºæ™¯

-   **ç ”ç©¶å¼€å‘**ï¼šéœ€è¦é«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ ç ”ç©¶
-   **ç”Ÿäº§éƒ¨ç½²**ï¼šå¯¹æ¨ç†æ€§èƒ½æœ‰é«˜è¦æ±‚çš„åº”ç”¨
-   **å¤§è§„æ¨¡è®­ç»ƒ**ï¼šå†…å­˜å’Œè®¡ç®—èµ„æºæœ‰é™çš„åœºæ™¯
-   **ç®—æ³•ä¼˜åŒ–**ï¼šéœ€è¦è‡ªå®šä¹‰é«˜æ€§èƒ½ç®—å­çš„åœºåˆ

é€šè¿‡ç†è§£ Jittor çš„å·¥ä½œæœºåˆ¶ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å…¶ä¼˜åŠ¿ï¼Œé¿å…å¸¸è§é—®é¢˜ï¼Œå®ç°é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ åº”ç”¨å¼€å‘ã€‚
