# python main.py --template Self_Blind_VSR_Realistic
import os
import torch
import multiprocessing
import warnings
import time
import platform
import psutil

# è®¾ç½®ä¸´æ—¶ç›®å½•ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—å‰æ‰§è¡Œï¼‰
# åˆ›å»ºä¸´æ—¶ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
temp_dir = 'D:\\TEMP'
if not os.path.exists(temp_dir):
    os.makedirs(temp_dir)
    print(f"âœ… ä¸´æ—¶ç›®å½•å·²åˆ›å»º: {temp_dir}")

# è®¾ç½®æ‰€æœ‰å¯èƒ½çš„ä¸´æ—¶ç›®å½•ç¯å¢ƒå˜é‡
os.environ['TEMP'] = temp_dir
os.environ['TMP'] = temp_dir
os.environ['TMPDIR'] = temp_dir
# CUDAç‰¹å®šçš„ä¸´æ—¶ç›®å½•
os.environ['CUDA_CACHE_PATH'] = os.path.join(temp_dir, 'cuda_cache')
if not os.path.exists(os.environ['CUDA_CACHE_PATH']):
    os.makedirs(os.environ['CUDA_CACHE_PATH'])

# è®¾ç½®PyTorchçš„ä¸´æ—¶ç›®å½•
torch.hub.set_dir(os.path.join(temp_dir, 'torch_hub'))

print(f"âš ï¸ ä¸´æ—¶ç›®å½•å·²å…¨éƒ¨é‡è®¾ä¸º: {temp_dir}")
print(f"   - TEMP = {os.environ['TEMP']}")
print(f"   - TMP = {os.environ['TMP']}")
print(f"   - CUDA_CACHE_PATH = {os.environ['CUDA_CACHE_PATH']}")

# å°è¯•ä½¿ç”¨CUDAå®ç°ï¼ˆå·²ä¿®æ”¹ä¸´æ—¶ç›®å½•ï¼Œé¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜ï¼‰
os.environ['FORCE_CORRELATION_PYTORCH'] = 'FALSE'
print("âœ… å°†å°è¯•ä½¿ç”¨CUDAç›¸å…³æ€§å®ç°ï¼ˆå¦‚ç¼–è¯‘å¤±è´¥ä¼šè‡ªåŠ¨å›é€€åˆ°PyTorchå®ç°ï¼‰")

import data
import model
import loss
import option
from trainer.trainer_flow_video import Trainer_Flow_Video
from logger import logger

warnings.filterwarnings("ignore")

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("\n====================================")
    print("ç³»ç»Ÿä¿¡æ¯")
    print("====================================")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.version()}")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"å¤„ç†å™¨: {platform.processor()}")
    
    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    print(f"ç³»ç»Ÿå†…å­˜: æ€»è®¡ {memory.total / (1024**3):.1f} GB, "
          f"å¯ç”¨ {memory.available / (1024**3):.1f} GB")
    
    # PyTorchå’ŒCUDAä¿¡æ¯
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"CUDAå¯ç”¨: æ˜¯ (ç‰ˆæœ¬ {torch.version.cuda})")
        print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    - å¯ç”¨æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory/1024**3:.1f} GB")
    else:
        print("CUDAå¯ç”¨: å¦ (ä»…CPUæ¨¡å¼å¯ç”¨)")
    print("====================================\n")

# æ·»åŠ Windowså¤šè¿›ç¨‹æ”¯æŒ
if __name__ == '__main__':
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    try:
        print_system_info()
    except Exception as e:
        print(f"æ— æ³•æ˜¾ç¤ºå®Œæ•´ç³»ç»Ÿä¿¡æ¯: {e}")

    start_time = time.time()
    
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼
    if torch.cuda.is_available():
        # CUDAè®¾å¤‡ä¸‹ä½¿ç”¨spawnæ–¹æ³•
        torch.multiprocessing.set_start_method('spawn', force=True)
    
    # æ­£å¸¸æ‰§è¡Œè®­ç»ƒæµç¨‹
    args = option.args
    
    # è°ƒæ•´è®­ç»ƒè®¾å¤‡è®¾ç½®
    if args.cpu:
        print("âš ï¸ å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼è¿è¡Œ")
        device = torch.device('cpu')
    else:
        if torch.cuda.is_available():
            cuda_idx = 0
            device = torch.device(f'cuda:{cuda_idx}')
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(cuda_idx)}")
            print(f"   - å¯ç”¨æ˜¾å­˜: {torch.cuda.get_device_properties(cuda_idx).total_memory/1024**3:.1f} GB")
            print(f"   - CUDAç‰ˆæœ¬: {torch.version.cuda}")
        else:
            device = torch.device('cpu')
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            args.cpu = True
    
    torch.manual_seed(args.seed)
    chkp = logger.Logger(args)

    print("\n====================================")
    print(f"ä»»åŠ¡: {args.task}")
    print(f"æ¨¡æ¿: {args.template}")
    print(f"æ•°æ®é›†: {args.data_train} (è®­ç»ƒ), {args.data_test} (æµ‹è¯•)")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å­¦ä¹ ç‡: {args.lr}")
    print(f"æ€»è®­ç»ƒè½®æ¬¡: {args.epochs}")
    print("====================================\n")
    
    try:
        if args.task == 'FlowVideoSR':
            print("ğŸ”„ å‡†å¤‡å¼€å§‹è®­ç»ƒä»»åŠ¡: {}".format(args.task))
            model = model.Model(args, chkp)
            print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            
            loss = loss.Loss(args, chkp) if not args.test_only else None
            if not args.test_only:
                print("âœ… æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
            
            print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®...")
            loader = data.Data(args)
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
            
            print("ğŸ”„ åˆå§‹åŒ–è®­ç»ƒå™¨...")
            t = Trainer_Flow_Video(args, loader, model, loss, chkp)
            print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
            
            train_start_time = time.time()
            while not t.terminate():
                t.train()
                t.test()
                
            total_train_time = time.time() - train_start_time
            print(f"\n====================================")
            print(f"è®­ç»ƒå®Œæˆï¼")
            print(f"æ€»è®­ç»ƒæ—¶é—´: {total_train_time/60:.2f}åˆ†é’Ÿ ({total_train_time/3600:.2f}å°æ—¶)")
            print(f"====================================\n")
        else:
            raise NotImplementedError('Task [{:s}] is not found'.format(args.task))
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    
    total_time = time.time() - start_time
    print(f"\næ€»è¿è¡Œæ—¶é—´: {total_time/60:.2f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)")
    chkp.done()
